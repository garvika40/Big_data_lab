{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVdalKrHBY1Y"
      },
      "source": [
        "Garvika singh rajawat\n",
        "202318061"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19LTh8xx_E0z",
        "outputId": "110230f3-3fc8-4c1a-c30e-db16b5a4dfae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=f20218bfb9b298275b21001b5b014fc2fbeff1536c0ccd5b4c4f790fd308474d\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z4dYS0R-Bd0n"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
        "from pyspark.sql.functions import corr\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
        "from pyspark.sql.functions import col\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaAdPd86B6iX"
      },
      "source": [
        "– TASK-1: Generate 100 random numbers in range 0 to 10 using numpy\n",
        "\n",
        "randint function with the seed set to 10. Create a RDD using the paral- lelize function using data generated in previous step. Calculate the fre- quency of each number (0 - 10) using appropriate function of RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cel0FwIRBzvg",
        "outputId": "fbb8855c-93e5-46ed-9714-f96cffccce58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number\t|\tFrequency\n",
            "-------------------------\n",
            "0\t|\t12\n",
            "1\t|\t11\n",
            "2\t|\t8\n",
            "3\t|\t6\n",
            "4\t|\t8\n",
            "5\t|\t5\n",
            "6\t|\t11\n",
            "7\t|\t5\n",
            "8\t|\t14\n",
            "9\t|\t12\n",
            "10\t|\t8\n"
          ]
        }
      ],
      "source": [
        "# Initialize Spark Context for analyzing number frequencies\n",
        "spark_context = SparkContext(appName=\"NumberFrequency\")\n",
        "\n",
        "# Set seed and generate 100 random numbers between 0 and 10\n",
        "np.random.seed(10)\n",
        "random_numbers = np.random.randint(0, 11, 100)\n",
        "\n",
        "# Create RDD from the generated numbers\n",
        "numbers_rdd = spark_context.parallelize(random_numbers)\n",
        "\n",
        "# Calculate the frequency of each number\n",
        "number_counts = numbers_rdd.countByValue()\n",
        "\n",
        "# Sort the frequency dictionary by key for better readability\n",
        "sorted_counts = dict(sorted(number_counts.items()))\n",
        "\n",
        "# Display the frequency of each number with a new format\n",
        "print(\"Number\\t|\\tFrequency\")\n",
        "print(\"-\" * 25)\n",
        "for number, count in sorted_counts.items():\n",
        "    print(f\"{number}\\t|\\t{count}\")\n",
        "\n",
        "# Stop the Spark Context to release resources\n",
        "spark_context.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRrDQ5RCCv3r"
      },
      "source": [
        "– TASK-2: In this task you will calculate the frequency of each word in text8 dataset mentioned above. Create a RDD using the text8 dataset.\n",
        "\n",
        "Use appropriate functions of the RDD to get the word frequencies. Fil- ter the RDD using appropriate function to get the frequencies of words\n",
        "\n",
        "containing the letter ’a’."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJaYfnorCsCy",
        "outputId": "96b56abe-6482-4c7b-eb52-61cbfab11c21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text data downloaded and extraction completed successfully.\n"
          ]
        }
      ],
      "source": [
        "# Source URL for acquiring the text data\n",
        "data_url = \"http://mattmahoney.net/dc/text8.zip\"\n",
        "\n",
        "# Setting the target directory for storing the compressed file\n",
        "target_directory = \"./dataset/\"\n",
        "\n",
        "# Ensure the existence of the specified directory\n",
        "Path(target_directory).mkdir(parents=True, exist_ok=True)\n",
        "compressed_file_path = Path(target_directory, \"text8_compressed.zip\")\n",
        "\n",
        "# Define the directory for extracted data\n",
        "extracted_data_dir = Path(target_directory, \"text8_data/\")\n",
        "\n",
        "# Ensure the designated extraction directory exists\n",
        "extracted_data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Fetching and downloading the text data from the provided source URL\n",
        "response = requests.get(data_url)\n",
        "compressed_file_path.write_bytes(response.content)\n",
        "\n",
        "# Unzipping and extracting the contents of the downloaded file\n",
        "with zipfile.ZipFile(compressed_file_path, 'r') as zip_file:\n",
        "    zip_file.extractall(extracted_data_dir)\n",
        "\n",
        "print(\"Text data downloaded and extraction completed successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cjJ7e9cDOWC",
        "outputId": "bb5576c6-6a7f-4894-ec8f-80ef77e8d684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations of what this means anarchism also refers to related social movements that advocate th\n"
          ]
        }
      ],
      "source": [
        "# Specify the path to the text file within the extracted folder\n",
        "text_file_path = extracted_data_dir / \"text8\"\n",
        "\n",
        "# Read the contents of the text file\n",
        "with open(text_file_path, 'r', encoding='utf-8') as text_file:\n",
        "    text_data = text_file.read()\n",
        "\n",
        "# Print the 500 characters of the text data\n",
        "print(text_data[200:700])\n",
        "\n",
        "# Stop the Spark Context\n",
        "spark_context.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCmLWvmCEb-z",
        "outputId": "81daab60-1c96-44c6-f4f7-b856ac4a579c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Covert Frequencies:\n",
            "Covert Code: a, Occurrences: 3\n",
            "Covert Code: pejorative, Occurrences: 1\n",
            "Covert Code: way, Occurrences: 1\n",
            "Covert Code: any, Occurrences: 1\n",
            "Covert Code: act, Occurrences: 1\n",
            "Covert Code: that, Occurrences: 3\n",
            "Covert Code: means, Occurrences: 2\n",
            "Covert Code: organization, Occurrences: 1\n",
            "Covert Code: has, Occurrences: 1\n",
            "Covert Code: also, Occurrences: 2\n",
            "Covert Code: taken, Occurrences: 1\n",
            "Covert Code: as, Occurrences: 2\n",
            "Covert Code: label, Occurrences: 1\n",
            "Covert Code: anarchists, Occurrences: 1\n",
            "Covert Code: anarchism, Occurrences: 3\n",
            "Covert Code: archons, Occurrences: 1\n",
            "Covert Code: political, Occurrences: 1\n",
            "Covert Code: are, Occurrences: 2\n",
            "Covert Code: unnecessary, Occurrences: 1\n",
            "Covert Code: and, Occurrences: 1\n",
            "Covert Code: abolished, Occurrences: 1\n",
            "Covert Code: although, Occurrences: 1\n",
            "Covert Code: interpretations, Occurrences: 1\n",
            "Covert Code: what, Occurrences: 1\n",
            "Covert Code: related, Occurrences: 1\n",
            "Covert Code: social, Occurrences: 1\n",
            "Covert Code: advocate, Occurrences: 1\n"
          ]
        }
      ],
      "source": [
        "# Check if a Spark context already exists, and stop it if it does\n",
        "if 'covert_context' in locals():\n",
        "    covert_context.stop()\n",
        "\n",
        "# Initialize the Spark Context for covert operations\n",
        "covert_context = SparkContext(\"local\", \"CovertOperationRDD\")\n",
        "\n",
        "# Create an RDD with encoded confidential information split into lines\n",
        "encoded_info_rdd = covert_context.parallelize(text_data[200:700].splitlines())\n",
        "\n",
        "# Split each line into fragments and filter those with the secret letter 'a'\n",
        "concealed_fragments_rdd = encoded_info_rdd.flatMap(lambda line: line.split(\" \")).filter(lambda fragment: 'a' in fragment)\n",
        "\n",
        "# Map each concealed fragment to a cryptic tuple with the fragment as the key and an undisclosed value of 1\n",
        "encrypted_fragments_rdd = concealed_fragments_rdd.map(lambda fragment: (fragment, 1))\n",
        "\n",
        "# Decrypt by key to reveal the frequency of each covert fragment\n",
        "decrypted_frequencies_rdd = encrypted_fragments_rdd.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Gather the results discreetly\n",
        "covert_frequency_results = decrypted_frequencies_rdd.collect()\n",
        "\n",
        "# Display the covert frequencies secretly\n",
        "print(\"Covert Frequencies:\")\n",
        "for covert_code, occurrences in covert_frequency_results:\n",
        "    print(f\"Covert Code: {covert_code}, Occurrences: {occurrences}\")\n",
        "\n",
        "# Conclude the covert operations Spark Context to maintain secrecy\n",
        "covert_context.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qagobq4EwHO",
        "outputId": "0d78aa2c-e413-4cf4-ccab-5980326933ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Covert Word Frequencies:\n",
            "Encoded Word: a, Occurrences: 3\n",
            "Encoded Word: pejorative, Occurrences: 1\n",
            "Encoded Word: way, Occurrences: 1\n",
            "Encoded Word: any, Occurrences: 1\n",
            "Encoded Word: act, Occurrences: 1\n",
            "Encoded Word: that, Occurrences: 3\n",
            "Encoded Word: means, Occurrences: 2\n",
            "Encoded Word: organization, Occurrences: 1\n",
            "Encoded Word: has, Occurrences: 1\n",
            "Encoded Word: also, Occurrences: 2\n",
            "Encoded Word: taken, Occurrences: 1\n",
            "Encoded Word: as, Occurrences: 2\n",
            "Encoded Word: label, Occurrences: 1\n",
            "Encoded Word: anarchists, Occurrences: 1\n",
            "Encoded Word: anarchism, Occurrences: 3\n",
            "Encoded Word: archons, Occurrences: 1\n",
            "Encoded Word: political, Occurrences: 1\n",
            "Encoded Word: are, Occurrences: 2\n",
            "Encoded Word: unnecessary, Occurrences: 1\n",
            "Encoded Word: and, Occurrences: 1\n",
            "Encoded Word: abolished, Occurrences: 1\n",
            "Encoded Word: although, Occurrences: 1\n",
            "Encoded Word: interpretations, Occurrences: 1\n",
            "Encoded Word: what, Occurrences: 1\n",
            "Encoded Word: related, Occurrences: 1\n",
            "Encoded Word: social, Occurrences: 1\n",
            "Encoded Word: advocate, Occurrences: 1\n"
          ]
        }
      ],
      "source": [
        "# Initialize the Spark Context for Stealthy Word Analysis\n",
        "word_analysis_context = SparkContext(\"local\", \"StealthyWordFrequencyTask\")\n",
        "\n",
        "# Create an RDD from the encoded text data\n",
        "encoded_text_rdd = word_analysis_context.parallelize(text_data[200:700].splitlines())\n",
        "\n",
        "# Split each line into encoded words and flatten the list\n",
        "encoded_word_list_rdd = encoded_text_rdd.flatMap(lambda line: line.split(\" \"))\n",
        "\n",
        "# Filter encoded words containing the secret character 'a'\n",
        "encoded_a_words_rdd = encoded_word_list_rdd.filter(lambda word: 'a' in word)\n",
        "\n",
        "# Map each encoded word to a tuple (word, 1) for further analysis\n",
        "encoded_word_tuples_rdd = encoded_a_words_rdd.map(lambda word: (word, 1))\n",
        "\n",
        "# Reduce by key to unveil the occurrences of each encoded word\n",
        "decoded_word_frequencies_rdd = encoded_word_tuples_rdd.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Collect the results discreetly\n",
        "covert_word_results = decoded_word_frequencies_rdd.collect()\n",
        "\n",
        "# Display the covert word frequencies covertly\n",
        "print(\"Covert Word Frequencies:\")\n",
        "for encoded_word, occurrences in covert_word_results:\n",
        "    print(f\"Encoded Word: {encoded_word}, Occurrences: {occurrences}\")\n",
        "\n",
        "# Conclude the Stealthy Word Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG4fxH1hE7af"
      },
      "source": [
        "• DataFrame Task: Create a Spark dataframe using the iris json data men- tioned above. Calculate Pearson Correlation between the columns petal- Length and petalWidth using the appropriate dataframe API. Show the\n",
        "\n",
        "columns sepalLength, sepalWidth and species for the rows of data that has petalLength greater than or equal to 1.4 using the appropriate dataframe API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2z79GdcE4IA"
      },
      "outputs": [],
      "source": [
        "ref_zip = zipfile.ZipFile('C:/Users/DSR/Desktop/new/archive (1)', 'r')\n",
        "ref_zip.extractall('C:/Users/DSR/Desktop/new/archive (1)')\n",
        "ref_zip.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Agjgiu3dFCeN"
      },
      "outputs": [],
      "source": [
        "iris_df=pd.read_csv('C:/Users/DSR/Desktop/new/archive (1)/iris.json')\n",
        "iris_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMJukS9QFGT4"
      },
      "outputs": [],
      "source": [
        "# Initiating the Spark Context for Floral Analysis\n",
        "floral_analysis_spark = SparkSession.builder.appName(\"FloralAnalysisTask\").getOrCreate()\n",
        "\n",
        "# Defining the schema for the iris dataset\n",
        "floral_schema = StructType([\n",
        "    StructField(\"sepalLength\", DoubleType(), True),\n",
        "    StructField(\"sepalWidth\", DoubleType(), True),\n",
        "    StructField(\"petalLength\", DoubleType(), True),\n",
        "    StructField(\"petalWidth\", DoubleType(), True),\n",
        "    StructField(\"species\", StringType(), True),\n",
        "])\n",
        "\n",
        "# Loading the iris dataset from the JSON file\n",
        "floral_data_df = floral_analysis_spark.read.json(\"/content/iris.json\", schema=floral_schema)\n",
        "\n",
        "# Displaying the first few rows of the Floral DataFrame\n",
        "floral_data_df.show(5)\n",
        "\n",
        "# Calculating the Pearson Correlation between petalLength and petalWidth\n",
        "petal_correlation = floral_data_df.stat.corr(\"petalLength\", \"petalWidth\")\n",
        "print(f\"PetalLength and PetalWidth Pearson Correlation: {petal_correlation:.4f}\")\n",
        "\n",
        "# Displaying selected columns for rows with petalLength >= 1.4\n",
        "selected_floral_df = floral_data_df.filter(col(\"petalLength\") >= 1.4).select(\"sepalLength\", \"sepalWidth\", \"species\")\n",
        "selected_floral_df.show()\n",
        "\n",
        "# Stopping the Spark session for floral analysis\n",
        "floral_analysis_spark.stop()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
